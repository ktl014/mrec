{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, plot_confusion_matrix, classification_report\n",
    "from mrec.data.dataset import load_data\n",
    "from mrec.features.transform import clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in training, validation data and labels\n",
    "csv_fnames = {'train': '../dataset/raw/train.csv', 'validation': '../dataset/raw/validation.csv',\n",
    "              'test': '../dataset/raw/test.csv'}\n",
    "dataset = load_data(csv_fnames)\n",
    "train, validation, test = dataset.train, dataset.validation, dataset.test\n",
    "\n",
    "relation_type = ['causes', 'treats']\n",
    "features_list = ['_unit_id', 'sentence', 'relation']\n",
    "train = train[features_list][train['relation'].isin(relation_type)].drop_duplicates()\n",
    "validation = validation[features_list][validation['relation'].isin(relation_type)].drop_duplicates()\n",
    "test = test[features_list][test['relation'].isin(relation_type)].drop_duplicates()\n",
    "\n",
    "#TODO add feature engineering\n",
    "count_vect = CountVectorizer(ngram_range=(1, 3), analyzer=clean_text)\n",
    "X_counts_train = count_vect.fit_transform(train['sentence'])\n",
    "X_train_label = train['relation']\n",
    "\n",
    "most_popular_word_df = pd.DataFrame(X_counts_train.toarray(), columns=count_vect.get_feature_names())\n",
    "\n",
    "X_counts_validation = count_vect.transform(validation['sentence'])\n",
    "X_validation_label = validation['relation']\n",
    "\n",
    "X_counts_test = count_vect.transform(test['sentence'])\n",
    "X_test_label = test['relation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame(most_popular_word_df.sum(axis=0), columns=['count']).sort_values(by='count', ascending=False)\n",
    "count_vec_top_200 = list(temp_df['count'][:200].index)\n",
    "count_vec_top_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metric(gtruth, predictions, dset_name):\n",
    "    \"\"\"Print 5 scoring metrics: accuracy, roc_auc, f1, precision, and recall\n",
    "\n",
    "    Args:\n",
    "        gtruth (array): label (either 0 or 1)\n",
    "        predictions (array): model prediction (either 0 or 1)\n",
    "        dset_name: the dataset that is evaluating on\n",
    "    \"\"\"\n",
    "    accuracy = round(accuracy_score(gtruth, predictions), 4)\n",
    "    roc_auc = round(roc_auc_score(gtruth, predictions), 4)\n",
    "    f1 = round(f1_score(gtruth, predictions), 4)\n",
    "    precision = round(precision_score(gtruth, predictions), 4)\n",
    "    recall = round(recall_score(gtruth, predictions), 4)\n",
    "    print('{:>10} {:>11} {:>12} {:>12} {:>11} {:>12}'.format(dset_name, accuracy, roc_auc, f1, precision, recall))\n",
    "\n",
    "def evaluate_model(model, X, y, dset_name):\n",
    "    \"\"\"Evaluate on given model\n",
    "\n",
    "    Args:\n",
    "        model: NuSVC()\n",
    "        X: countvectorizers of feature(s)\n",
    "        y: label\n",
    "        dset_name: dataset that is evaluating on\n",
    "    \"\"\"\n",
    "    enc = LabelEncoder()\n",
    "\n",
    "    predictions = model.predict(X)\n",
    "    gtruth = enc.fit_transform(y)\n",
    "    encoder_predictions = enc.transform(predictions)\n",
    "\n",
    "    print_metric(gtruth, encoder_predictions, dset_name)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train the best model\"\"\"\n",
    "model = NuSVC()\n",
    "\n",
    "print('Training model..')\n",
    "model.fit(X_counts_train, X_train_label)\n",
    "\n",
    "print('{:>23} {:>12} {:>12} {:>12} {:>10}'.format('Accuracy', 'ROC_AUC', 'F1-score', 'Precision', 'Recall'))\n",
    "train_predictions = evaluate_model(model, X_counts_train, X_train_label, 'Train')\n",
    "\n",
    "val_predictions = evaluate_model(model, X_counts_validation, X_validation_label, 'Validation')\n",
    "\n",
    "test_predictions = evaluate_model(model, X_counts_test, X_test_label, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = X_test_label.unique()\n",
    "plot_confusion_matrix(model, X_counts_test, X_test_label,\n",
    "                      display_labels=class_names,\n",
    "                      cmap=plt.cm.Blues,\n",
    "                      normalize='true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "gtruth = enc.fit_transform(X_test_label)\n",
    "encoder_predictions = enc.transform(test_predictions)\n",
    "print(classification_report(gtruth, encoder_predictions, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Users/ktle2/personal_projects/mrec/models/baseline_model'\n",
    "# add predictions back into the dataframe\n",
    "# save the dataframe as the csv file `-predictions.csv`\n",
    "\n",
    "train['relation_pred'] = train_predictions\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pred = train.set_index('_unit_id').relation_pred.to_dict()\n",
    "train_with_pred = dataset.train\n",
    "train_with_pred['relation_pred'] = train_with_pred['_unit_id'].map(id_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "mode = 'train'\n",
    "csv_file = os.path.join(base_dir, '{}-predictions.csv')\n",
    "train_with_pred.to_csv(csv_file.format(mode))\n",
    "print('File saved {}'.format(csv_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(data, predictions, new_data, save=False, output_csv_file=None):\n",
    "    pred_col = 'relation_pred'\n",
    "    id_col = '_unit_id'\n",
    "    data[pred_col] = predictions\n",
    "    id_pred = data.set_index(id_col).relation_pred.to_dict()\n",
    "    new_data_with_pred = new_data\n",
    "    new_data_with_pred['relation_pred'] = new_data_with_pred['_unit_id'].map(id_pred)\n",
    "    if save:\n",
    "        print(f'Saving file as {output_csv_file}')\n",
    "        new_data_with_pred.to_csv(output_csv_file)\n",
    "    return new_data_with_pred\n",
    "\n",
    "val_predd = save_predictions(validation, val_predictions, dataset.validation, save=True, output_csv_file=csv_file.format('validation'))\n",
    "test_predd = save_predictions(test, test_predictions, dataset.test, save=True, output_csv_file=csv_file.format('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_fnames = {'train': 'dataset/raw/train.csv', 'validation': 'dataset/raw/validation.csv',\n",
    "              'test': 'dataset/raw/test.csv'}\n",
    "relation_type = ['causes', 'treats']\n",
    "dataset = load_data(csv_fnames)\n",
    "train_df, validation_df, test_df = dataset.train, dataset.validation, dataset.test\n",
    "\n",
    "train_df = train_df[['sentence', 'relation']][train_df['relation'].isin(relation_type)].drop_duplicates()\n",
    "validation_df = validation_df[['sentence', 'relation']][validation_df['relation'].isin(relation_type)].drop_duplicates()\n",
    "test_df = test_df[['sentence', 'relation']][test_df['relation'].isin(relation_type)].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasson why model fails to classify relation in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['pred'] = predictions\n",
    "false_pred_df = test_df[test_df['relation'] != test_df['pred']]\n",
    "false_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pred_df.iloc[1].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = dataset.train[['_unit_id', 'relation', 'sentence', 'direction', 'term1', 'term2']]\n",
    "raw[raw['sentence'] == \"INSULIN PEPTIDE B9 23 is a major autoantigen in TYPE 1 DIABETES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reason why model fails to classify in validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df['pred'] = val_predictions\n",
    "validation_df_false_pred = validation_df[validation_df['pred'] != validation_df['relation']]\n",
    "validation_df_false_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df_false_pred.iloc[0].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = dataset.validation[['_unit_id', 'relation', 'sentence', 'direction', 'term1', 'term2']]\n",
    "raw_df[raw_df['sentence'] == 'A possible role of LEU in sensomotor cortex is limitation of intensity and duration of SEIZURES and prevention of STATUS EPILEPTICUS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = dataset.train[['_unit_id', 'relation', 'sentence', 'direction', 'term1', 'term2']]\n",
    "raw_df[raw_df['sentence'] == 'A possible role of LEU in sensomotor cortex is limitation of intensity and duration of SEIZURES and prevention of STATUS EPILEPTICUS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 7000)\n",
    "count_vect_df = pd.DataFrame(test.toarray(), columns=count_vect.get_feature_names())\n",
    "count_vect_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate length of message excluding space\n",
    "train_df['chacracter count'] = train_df['sentence'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "train_df['word count'] = train_df['sentence'].apply(lambda x: len(x.split()))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 600, 60)\n",
    "\n",
    "plt.hist(train_df[train_df['relation']=='causes']['chacracter count'], bins, alpha=0.5, label='causes', density=True)\n",
    "plt.hist(train_df[train_df['relation']=='treats']['chacracter count'], bins, alpha=0.5, label='treats', density=True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pred_df['chacracter count'] = false_pred_df['sentence'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "bins = np.linspace(0, 600, 60)\n",
    "\n",
    "plt.hist(false_pred_df[false_pred_df['relation']=='causes']['chacracter count'], bins, alpha=0.5, label='causes', density=True)\n",
    "plt.hist(false_pred_df[false_pred_df['relation']=='treats']['chacracter count'], bins, alpha=0.5, label='treats', density=True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 100, 60)\n",
    "\n",
    "plt.hist(train_df[train_df['relation']=='causes']['word count'], bins, alpha=0.5, label='causes', density=True)\n",
    "plt.hist(train_df[train_df['relation']=='treats']['word count'], bins, alpha=0.5, label='treats', density=True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pred_df['word count'] = false_pred_df['sentence'].apply(lambda x: len(x.split()))\n",
    "bins = np.linspace(0, 100, 60)\n",
    "\n",
    "plt.hist(false_pred_df[false_pred_df['relation']=='causes']['word count'], bins, alpha=0.5, label='causes', density=True)\n",
    "plt.hist(false_pred_df[false_pred_df['relation']=='treats']['word count'], bins, alpha=0.5, label='treats', density=True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "train_df['punct%'] = train_df['sentence'].apply(lambda x: count_punct(x))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 20, 40)\n",
    "\n",
    "plt.hist(train_df[train_df['relation']=='causes']['punct%'], bins, alpha=0.5, label='causes', density=True)\n",
    "plt.hist(train_df[train_df['relation']=='treats']['punct%'], bins, alpha=0.5, label='treats', density=True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF vs CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(ngram_range=(1, 3), analyzer=clean_text, use_idf=False)\n",
    "X_tfidf_train = tfidf_vect.fit_transform(train_df['sentence'])\n",
    "X_tfidf_train_label = train_df['relation']\n",
    "\n",
    "'''\n",
    "indices = np.argsort(tfidf_vect.idf_)[::-1]\n",
    "features = tfidf_vect.get_feature_names()\n",
    "top_n = 200\n",
    "top_features = [features[i] for i in indices[:top_n]]\n",
    "print(top_features)\n",
    "'''\n",
    "\n",
    "X_tfidf_eval = tfidf_vect.transform(validation_df['sentence'])\n",
    "X_tfidf_eval_label = validation_df['relation']\n",
    "\n",
    "X_tfidf_test = tfidf_vect.transform(test_df['sentence'])\n",
    "X_tfidf_test_label = test_df['relation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = NuSVC()\n",
    "\n",
    "print('Training model..')\n",
    "new_model.fit(X_tfidf_train, X_tfidf_train_label)\n",
    "\n",
    "print('{:>23} {:>12} {:>12} {:>12} {:>10}'.format('Accuracy', 'ROC_AUC', 'F1-score', 'Precision', 'Recall'))\n",
    "evaluate_model(new_model, X_tfidf_train, X_tfidf_train_label, 'Train')\n",
    "\n",
    "evaluate_model(new_model, X_tfidf_eval, X_tfidf_eval_label, 'Validation')\n",
    "\n",
    "predictions = evaluate_model(new_model, X_tfidf_test, X_tfidf_test_label, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "gtruth = enc.fit_transform(X_tfidf_test_label)\n",
    "encoder_predictions = enc.transform(predictions)\n",
    "print(classification_report(gtruth, encoder_predictions, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = X_tfidf_test_label.unique()\n",
    "plot_confusion_matrix(new_model, X_tfidf_test, X_tfidf_test_label,\n",
    "                      display_labels=class_names,\n",
    "                      cmap=plt.cm.Blues,\n",
    "                      normalize='true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrec",
   "language": "python",
   "name": "mrec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
