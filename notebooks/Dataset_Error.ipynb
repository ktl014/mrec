{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here I will show there are overlapping sentences in the train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mrec.data.dataset import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_fnames = {'train': 'dataset/raw/train.csv', 'validation': 'dataset/raw/validation.csv', 'test': 'dataset/raw/test.csv'}\n",
    "\n",
    "base_dir = '/Users/ktle2/personal_projects/mrec/models/baseline_model'\n",
    "pred_csv_fnames = {'train': f'{base_dir}/train-predictions.csv',\n",
    "                   'validation': f'{base_dir}/validation-predictions.csv',\n",
    "                   'test': f'{base_dir}/test-predictions.csv'}\n",
    "\n",
    "dataset = load_data(pred_csv_fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['_unit_id', 'relation', 'sentence', 'direction', 'term1', 'term2', 'relation_pred']\n",
    "train = dataset.train[cols]\n",
    "validation = dataset.validation[cols]\n",
    "test= dataset.test[cols]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here I will show the inconsistent in labeling `relation` on sentences. I will group `_unit_id`, `relation`, `sentence`, `term1`, and `term2` and do a majority vote on `direction` to remove duplicates. Then I will show that same sentences can have different relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAJORITY_VOTE_FLAG = False\n",
    "# majority vote assigned as the direction, thereby duplicates are removed\n",
    "if MAJORITY_VOTE_FLAG:\n",
    "    group_cols = ['_unit_id', 'relation', 'sentence', 'term1', 'term2', 'relation_pred']\n",
    "    train_no_dup = train.groupby(group_cols)['direction'].agg(pd.Series.mode).reset_index()\n",
    "    val_no_dup = validation.groupby(group_cols)['direction'].agg(pd.Series.mode).reset_index()\n",
    "    test_no_dup = test.groupby(group_cols)['direction'].agg(pd.Series.mode).reset_index()\n",
    "else:\n",
    "    relation_type = ['causes', 'treats']\n",
    "    train_no_dup = train[train['relation'].isin(relation_type)].drop_duplicates(subset='_unit_id')\n",
    "    val_no_dup = validation[validation['relation'].isin(relation_type)].drop_duplicates(subset='_unit_id')\n",
    "    test_no_dup = test[test['relation'].isin(relation_type)].drop_duplicates(subset='_unit_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_dup['sentence'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = train_no_dup.groupby(['sentence']).size().reset_index(name='show-up counts')\n",
    "print(grouped_df.shape)\n",
    "duplicated_sentences_count = grouped_df[grouped_df['show-up counts'] > 1].reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of duplicated sentences within training set: {duplicated_sentences_count.shape[0]}\")\n",
    "print(f\"Distribution of duplicated sentences:\\n{duplicated_sentences_count['show-up counts'].value_counts()}\")\n",
    "duplicated_sentences_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We see that we still have duplicated sentences. Let's look close to sentence that have 3 duplicates after doing majority vote__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '164  Babesiosis  Treatment of BABESIOSIS   +    caused by  BABESIA MICROTI.'\n",
    "train_no_dup[train_no_dup['sentence'] == sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This sentence have duplicates beucase it has different `_unit_id` and `relation`. If we do majority vote without grouping `_unit_id`, we still have sentence duplicated and have different relation. Hence this train dataset is inconsistent in labeling relation for each unique sentence__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_size = train_no_dup.shape[0]\n",
    "duplicates = dset_size - train_no_dup['sentence'].nunique()\n",
    "print('Number of rows after do majority vote:', dset_size)\n",
    "print('Number of duplicate sentences:', duplicates)\n",
    "print('Normalize: {:.2f}%'.format(duplicates / dset_size * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_train_predictions = train_no_dup[train_no_dup['relation'] != train_no_dup['relation_pred']]\n",
    "misclassified_duplicated_sentences_total = false_train_predictions.shape[0]\n",
    "print('Misclassified duplicated sentences: {}({:0.3f}%)\\n'.format(misclassified_duplicated_sentences_total, misclassified_duplicated_sentences_total/dset_size*100))\n",
    "\n",
    "print('Verifying that sentence is repeated within training sentence')\n",
    "sample_sentence = 'Thus, the present data support the hypothesis that the therapeutic effects of CLOZAPINE in this primate model and perhaps in SCHIZOPHRENIA may be related at least in part to the restoration of DA tone in the prefrontal cortex.'\n",
    "train_no_dup[train_no_dup['sentence'] == sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_no_dup.shape)\n",
    "clean_train = train_no_dup.drop(list(train_no_dup[train_no_dup['sentence'].duplicated(False)].index))\n",
    "print(clean_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train[clean_train['sentence'] == sample_sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Here is what that sentence look like in raw train set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['sentence'] == sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Here is how severe this case is in validation set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = val_no_dup['sentence'].duplicated().sum()\n",
    "dset_size = val_no_dup.shape[0]\n",
    "print('Number of rows after do majority vote:', dset_size)\n",
    "print('Number of duplicate sentences:', duplicates)\n",
    "print('Normalize: {:.2f}%'.format(duplicates / dset_size * 100))\n",
    "\n",
    "\n",
    "false_val_predictions = val_no_dup[val_no_dup['relation'] != val_no_dup['relation_pred']]\n",
    "misclassified_duplicated_sentences_total = false_val_predictions.shape[0]\n",
    "print('Misclassified duplicated sentences: {}({:0.3f}%)\\n'.format(misclassified_duplicated_sentences_total, misclassified_duplicated_sentences_total/dset_size*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Here is how severe this case is in test set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = test_no_dup['sentence'].duplicated().sum()\n",
    "dset_size = test_no_dup.shape[0]\n",
    "print('Number of rows after do majority vote:', dset_size)\n",
    "print('Number of duplicate sentences:', duplicates)\n",
    "print('Normalize: {:.2f}%'.format(duplicates / dset_size * 100))\n",
    "\n",
    "\n",
    "false_test_predictions = test_no_dup[test_no_dup['relation'] != test_no_dup['relation_pred']]\n",
    "misclassified_duplicated_sentences_total = false_test_predictions.shape[0]\n",
    "print('Misclassified duplicated sentences: {}({:0.3f}%)\\n'.format(misclassified_duplicated_sentences_total, misclassified_duplicated_sentences_total/dset_size*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CasIn order to prove that there are overlapping sentences in train, validation and test set, I will do majority vote on `direction` in each set to remove duplicates. Then I will concatenate train and validation set and check for duplicate sentences. I will also concatenate train and test set and check for duplicate sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_val_dfs = [train_no_dup, val_no_dup]\n",
    "train_concat_val = pd.concat(train_and_val_dfs)\n",
    "train_concat_val['sentence'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['tom'], ['tom'], ['tom']]\n",
    "df = pd.DataFrame(data, columns=['Name'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrec",
   "language": "python",
   "name": "mrec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
